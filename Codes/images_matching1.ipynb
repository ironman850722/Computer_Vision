{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image retrieval by matching keypoints\n"
    "### Detect 20 strongest SIFT keypoints in the image.\n",
    "### Extract 20 image patches of size 32x32 pixels, centered at the detected SIFT keypoints, from the image\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import natsort\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_retrieval/query/q1.JPG\n",
      "images_retrieval/query/q2.JPG\n",
      "images_retrieval/query/q3.JPG\n",
      "images_retrieval/query/q4.JPG\n",
      "images_retrieval/query/q5.JPG\n",
      "images_retrieval/query/q6.JPG\n",
      "images_retrieval/query/q7.JPG\n",
      "images_retrieval/query/q8.JPG\n",
      "images_retrieval/query/q9.JPG\n",
      "images_retrieval/query/q10.JPG\n",
      "images_retrieval/query/q11.JPG\n",
      "images_retrieval/query/q12.JPG\n",
      "images_retrieval/query/q13.JPG\n",
      "images_retrieval/query/q14.JPG\n",
      "images_retrieval/query/q15.JPG\n",
      "images_retrieval/query/q16.JPG\n",
      "images_retrieval/query/q17.JPG\n",
      "images_retrieval/query/q18.JPG\n",
      "images_retrieval/query/q19.JPG\n",
      "images_retrieval/query/q20.JPG\n",
      "images_retrieval/query/q21.JPG\n",
      "images_retrieval/query/q22.JPG\n",
      "images_retrieval/query/q23.JPG\n",
      "images_retrieval/query/q24.JPG\n",
      "images_retrieval/query/q25.JPG\n",
      "images_retrieval/query/q26.JPG\n",
      "images_retrieval/query/q27.JPG\n",
      "images_retrieval/query/q28.JPG\n",
      "images_retrieval/query/q29.JPG\n",
      "images_retrieval/query/q30.JPG\n",
      "images_retrieval/query/q31.JPG\n",
      "images_retrieval/query/q32.JPG\n",
      "images_retrieval/query/q33.JPG\n",
      "images_retrieval/query/q34.JPG\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"images_retrieval/query\"\n",
    "total_images = 0\n",
    "all_kp_s = []\n",
    "all_kp_h = []\n",
    "if os.path.exists(img_dir):\n",
    "    if os.listdir(img_dir) is []:\n",
    "        print(\"No images!\")\n",
    "        exit(0)\n",
    "    num_img = len(os.listdir(img_dir))\n",
    "    \n",
    "    filename = os.listdir(img_dir)\n",
    "    sorted_file = natsort.natsorted(filename)\n",
    "    \n",
    "    for img in sorted_file:\n",
    "        if not img.endswith(\"JPG\"):\n",
    "            continue\n",
    "        total_images = total_images + 1\n",
    "        images_dir = os.path.join(img_dir, img)\n",
    "        print(images_dir)\n",
    "        images = cv2.imread(images_dir)\n",
    "        gray= cv2.cvtColor(images,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # SIFT Detection\n",
    "        sift = cv2.SIFT_create(nfeatures = 20)\n",
    "        kp = []\n",
    "        kp = sift.detect(gray,None)      #kp[0] = x coordinate, kp[1] = y coordinate\n",
    "        #print(kp)\n",
    "        all_kp_s.append(kp)\n",
    "        \n",
    "        img1 = cv2.drawKeypoints(gray,kp,images )\n",
    "        #cv2.imwrite('test/SIFT_keypoints' + str(img),images)\n",
    "        \n",
    "        #plt.imshow(images)\n",
    "        #plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"image folder not exists!\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatches(kps, img, size=32, num=500):\n",
    "    res = torch.zeros(num, 1, size, size)\n",
    "    if type(img) is np.ndarray:\n",
    "        img = torch.from_numpy(img)\n",
    "    h, w = img.shape      # note: for image, the x direction is the verticle, y-direction is the horizontal...\n",
    "    for i in range(num):\n",
    "        cx, cy = kps[i]\n",
    "        cx, cy = int(cx), int(cy)\n",
    "        dd = int(size/2)\n",
    "        xmin, xmax = max(0, cx - dd), min(w, cx + dd ) \n",
    "        ymin, ymax = max(0, cy - dd), min(h, cy + dd ) \n",
    "        \n",
    "        xmin_res, xmax_res = dd - min(dd,cx), dd + min(dd, w - cx)\n",
    "        ymin_res, ymax_res = dd - min(dd,cy), dd + min(dd, h - cy)\n",
    "\n",
    "        cropped_img = img[ymin: ymax, xmin: xmax]\n",
    "        ch, cw = cropped_img.shape\n",
    "        res[i, 0, ymin_res: ymin_res+ch, xmin_res: xmin_res+cw] =  cropped_img\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot these patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_retrieval/query/q1.JPG\n",
      "images_retrieval/query/q2.JPG\n",
      "images_retrieval/query/q3.JPG\n",
      "images_retrieval/query/q4.JPG\n",
      "images_retrieval/query/q5.JPG\n",
      "images_retrieval/query/q6.JPG\n",
      "images_retrieval/query/q7.JPG\n",
      "images_retrieval/query/q8.JPG\n",
      "images_retrieval/query/q9.JPG\n",
      "images_retrieval/query/q10.JPG\n",
      "images_retrieval/query/q11.JPG\n",
      "images_retrieval/query/q12.JPG\n",
      "images_retrieval/query/q13.JPG\n",
      "images_retrieval/query/q14.JPG\n",
      "images_retrieval/query/q15.JPG\n",
      "images_retrieval/query/q16.JPG\n",
      "images_retrieval/query/q17.JPG\n",
      "images_retrieval/query/q18.JPG\n",
      "images_retrieval/query/q19.JPG\n",
      "images_retrieval/query/q20.JPG\n",
      "images_retrieval/query/q21.JPG\n",
      "images_retrieval/query/q22.JPG\n",
      "images_retrieval/query/q23.JPG\n",
      "images_retrieval/query/q24.JPG\n",
      "images_retrieval/query/q25.JPG\n",
      "images_retrieval/query/q26.JPG\n",
      "images_retrieval/query/q27.JPG\n",
      "images_retrieval/query/q28.JPG\n",
      "images_retrieval/query/q29.JPG\n",
      "images_retrieval/query/q30.JPG\n",
      "images_retrieval/query/q31.JPG\n",
      "images_retrieval/query/q32.JPG\n",
      "images_retrieval/query/q33.JPG\n",
      "images_retrieval/query/q34.JPG\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "#img = cv2.imread('images/5.jpg')\n",
    "#gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# gray = img[:, :, 0]\n",
    "\n",
    "all_patches = []\n",
    "all_coordinates = []\n",
    "order = 0 # go through keypoints in order\n",
    "\n",
    "for img in sorted_file:\n",
    "    Mykeypoints = []\n",
    "    if not img.endswith(\"JPG\"):\n",
    "        continue\n",
    "    images_dir = os.path.join(img_dir, img)\n",
    "    print(images_dir)\n",
    "    images = cv2.imread(images_dir)\n",
    "    gray= cv2.cvtColor(images,cv2.COLOR_BGR2GRAY)    \n",
    "    \n",
    "    \n",
    "    #SIFT\n",
    "    for numkp in range(20):        \n",
    "        Mykeypoints = Mykeypoints + [ (all_kp_s[order][numkp].pt[0], all_kp_s[order][numkp].pt[1]) ]\n",
    "    order = order + 1\n",
    "    all_coordinates.append(Mykeypoints)    \n",
    "    patches = getPatches(Mykeypoints, gray,size=32, num=20)    \n",
    "    all_patches.append(patches)        \n",
    "print(len(all_patches))        \n",
    "#for patch in patches:\n",
    "#    im = patch[0].numpy()\n",
    "#    plt.imshow(im)\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the patches with PyTorch\n",
    "For each image, you can output the patches within one tensor. In above examples, tensor ***patches*** is the one that you should store in a list. And then save the list as a \"SIFT.pth\" file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_patches = []\n",
    "#all_patches.append(patches)\n",
    "\n",
    "\n",
    "#all_coordinates = torch.FloatTensor(all_coordinates)\n",
    "#print(len(all_coordinates))\n",
    "#output_dir1 = \"SIFT.pth\"         # modify it to SIFT.pth or Harris.pth\n",
    "#torch.save(all_coordinates, output_dir1)\n",
    "\n",
    "\n",
    "all_patches = torch.stack(all_patches, dim=0)\n",
    "output_dir2 = \"cd .. My_Results/Query_Images_Matching/Query_patches.pth\"         # modify it to SIFT.pth or Harris.pth\n",
    "torch.save(all_patches, output_dir2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with your saved patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([34, 20, 1, 32, 32])\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "#test_coordinates = torch.load(output_dir1)\n",
    "#print(type(test_coordinates))\n",
    "#print(test_coordinates.shape)\n",
    "\n",
    "test_patches = torch.load(output_dir2)\n",
    "print(type(test_patches))\n",
    "print(test_patches.shape)\n",
    "print(len(test_patches))\n",
    "# your tensor for each should have size of [10, 200, 1, 32, 32];where 10 means 10 images (in the order 1-10), 200 means 200 points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
