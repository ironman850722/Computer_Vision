{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint(Patch) Description\n",
    "<a id='top_cell'></a>  \n",
    "This project will be all about defining and training a convolutional neural network to perform keypoint  description. \n",
    "PyTorch tutorials are available at here: [pytorch tutorials](https://github.com/yunjey/pytorch-tutorial)\n",
    "\n",
    "Today we will go through:\n",
    "### 1. [Load and visualize the data](#load_data_cell)\n",
    "### 2. [Build an example deep network](#build_network_cell)\n",
    "### 3. [Train the deep network](#train_network_cell)\n",
    "### 4. [Generate deep features](#generate_deep_features_cell)\n",
    "\n",
    "***\n",
    "\n",
    "We will use below dataset in this project:\n",
    "###  [The Photo Tourism dataset ](http://phototour.cs.washington.edu/patches/default.htm)\n",
    "\n",
    "It is also available in PyTorch torchvision datasets: [pytorch version](https://pytorch.org/docs/stable/_modules/torchvision/datasets/phototour.html#PhotoTour)\n",
    "\n",
    "This dataset consists of 1024 x 1024 bitmap (.bmp) images, each containing a 16 x 16 array of image patches. Here are some examples:\n",
    "\n",
    "<table><tr><td><img src='images/patches0001.bmp' width=68% ></td><td><img src='images/patches1482.bmp' width=68%></td></tr></table>    \n",
    "For details of how the scale and orientation is established, please see the paper:  \n",
    "<p class=\"style8\"><font size=\"2\">S. Winder and M. Brown. <strong>Learning Local Image \n",
    "\t\t\t\tDescriptors</strong>. To appear <i>International Conference on \n",
    "\t\t\t\tComputer Vision and Pattern Recognition (CVPR2007)</i> (</font><a href=\"http://research.microsoft.com/~swinder/papers/winder_brown_cvpr07.pdf\"><span class=\"style9\">pdf \n",
    "\t\t\t\t300Kb</span></a><font size=\"2\">)</font></p>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64\n",
    "from Utils import L2Norm, cv2_scale, np_reshape\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability, using nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Since there are two GPUs on each pelican server, you can either select it as 0 or 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<a id='load_data_cell'></a>\n",
    "## Load and visualize the data\n",
    "\n",
    "In this section, we will \n",
    "#### 1. [Define a PyTorch dataset](#pytorch_dataset_cell)\n",
    "#### 2. [Define a PyTorch dataloader](#pytorch_dataloader_cell)\n",
    "#### 3. [Load data](#load_dataset_cell)\n",
    "#### 4. [Visualizaiton of the Training and Testing Data](#visualize_dataset_cell)\n",
    "\n",
    "[BackToTop](#top_cell)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_dataset_cell'></a>\n",
    "### Define PyTorch dataset\n",
    "\n",
    "[BackToSection](#load_data_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletPhotoTour(dset.PhotoTour):\n",
    "    \"\"\"\n",
    "    From the PhotoTour Dataset it generates triplet samples\n",
    "    note: a triplet is composed by a pair of matching images and one of\n",
    "    different class.\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, transform=None, batch_size = None,load_random_triplets = False,  *arg, **kw):\n",
    "        super(TripletPhotoTour, self).__init__(*arg, **kw)\n",
    "        self.transform = transform\n",
    "        self.out_triplets = load_random_triplets\n",
    "        self.train = train\n",
    "        self.n_triplets = args.n_triplets\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n",
    "            \n",
    "    @staticmethod\n",
    "    def generate_triplets(labels, num_triplets):\n",
    "        def create_indices(_labels):\n",
    "            inds = dict()\n",
    "            for idx, ind in enumerate(_labels):\n",
    "                if ind not in inds:\n",
    "                    inds[ind] = []\n",
    "                inds[ind].append(idx)\n",
    "            return inds\n",
    "\n",
    "        triplets = []\n",
    "        indices = create_indices(labels.numpy())\n",
    "        unique_labels = np.unique(labels.numpy())\n",
    "        n_classes = unique_labels.shape[0]\n",
    "        # add only unique indices in batch\n",
    "        already_idxs = set()\n",
    "\n",
    "        for x in tqdm(range(num_triplets)):\n",
    "            if len(already_idxs) >= args.batch_size:\n",
    "                already_idxs = set()\n",
    "            c1 = np.random.randint(0, n_classes)\n",
    "            while c1 in already_idxs:\n",
    "                c1 = np.random.randint(0, n_classes)\n",
    "            already_idxs.add(c1)\n",
    "            c2 = np.random.randint(0, n_classes)\n",
    "            while c1 == c2:\n",
    "                c2 = np.random.randint(0, n_classes)\n",
    "            if len(indices[c1]) == 2:  # hack to speed up process\n",
    "                n1, n2 = 0, 1\n",
    "            else:\n",
    "                n1 = np.random.randint(0, len(indices[c1]))\n",
    "                n2 = np.random.randint(0, len(indices[c1]))\n",
    "                while n1 == n2:\n",
    "                    n2 = np.random.randint(0, len(indices[c1]))\n",
    "            n3 = np.random.randint(0, len(indices[c2]))\n",
    "            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def transform_img(img):\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img.numpy())\n",
    "            return img\n",
    "\n",
    "        t = self.triplets[index]\n",
    "        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n",
    "\n",
    "        img_a = transform_img(a)\n",
    "        img_p = transform_img(p)\n",
    "        img_n = None\n",
    "        if self.out_triplets:\n",
    "            img_n = transform_img(n)\n",
    "        # transform images if required\n",
    "        if args.fliprot:\n",
    "            do_flip = random.random() > 0.5\n",
    "            do_rot = random.random() > 0.5\n",
    "            if do_rot:\n",
    "                img_a = img_a.permute(0,2,1)\n",
    "                img_p = img_p.permute(0,2,1)\n",
    "                if self.out_triplets:\n",
    "                    img_n = img_n.permute(0,2,1)\n",
    "            if do_flip:\n",
    "                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n",
    "                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n",
    "                if self.out_triplets:\n",
    "                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n",
    "        return (img_a, img_p, img_n)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triplets.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_dataloader_cell'></a>\n",
    "### Define the dataloader\n",
    "[BackToSection](#load_data_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset_names, load_random_triplets = False, verbose=False):\n",
    "    \"\"\"\n",
    "    For training, we use dataset 'liberty';\n",
    "    For testing, we use dataset 'notredame' and 'yosemite'\n",
    "    \n",
    "    \"\"\"\n",
    "    test_dataset_names = copy(dataset_names)\n",
    "    test_dataset_names.remove(args.training_set)\n",
    "\n",
    "    kwargs = {'num_workers': args.num_workers, 'pin_memory': args.pin_memory} if args.cuda else {}\n",
    "\n",
    "    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n",
    "            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Lambda(cv2_scale),\n",
    "            transforms.Lambda(np_reshape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((args.mean_image,), (args.std_image,))])\n",
    "    if not args.augmentation:\n",
    "        transform_train = transform\n",
    "        transform_test = transform\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            TripletPhotoTour(train=True,\n",
    "                             load_random_triplets = load_random_triplets,\n",
    "                             batch_size=args.batch_size,\n",
    "                             root=args.dataroot,\n",
    "                             name=args.training_set,\n",
    "                             download=True,\n",
    "                             transform=transform_train),\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, **kwargs)\n",
    "\n",
    "    test_loaders = [{'name': name,\n",
    "                     'dataloader': torch.utils.data.DataLoader(\n",
    "             TripletPhotoTour(train=False,\n",
    "                     batch_size=args.test_batch_size,\n",
    "                     load_random_triplets = load_random_triplets,\n",
    "                     root=args.dataroot,\n",
    "                     name=name,\n",
    "                     download=True,\n",
    "                     transform=transform_test),\n",
    "                        batch_size=args.test_batch_size,\n",
    "                        shuffle=False, **kwargs)}\n",
    "                    for name in test_dataset_names]\n",
    "\n",
    "    return train_loader, test_loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<a id='build_network_cell'></a>\n",
    "## Build an exmaple deep network  \n",
    "\n",
    "In this section, we will:\n",
    "#### 1. [Build the deep network: DesNet](#build_desNet_cell)\n",
    "#### 2. [Setup optimization](#set_opt_cell)\n",
    "\n",
    "[BackToTop](#top_cell)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='build_desNet_cell'></a>\n",
    "### Build the deep network: DesNet\n",
    "The DesNet is a simple CNN network, which only contains two CNN blocks.\n",
    "\n",
    "\n",
    "[BackToSection](#build_network_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DesNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load network from the python file. You need to submit these .py files to TA\n",
    "from CNN1 import DesNet       # uncomment this line if you are using DesNet from CNN1.py\n",
    "#from CNN2 import DesNet      # uncomment this line if you are using DesNet from CNN2.py\n",
    "#from CNN3 import DesNet\n",
    "#from class_CNN3 import DesNet      # uncomment this line if you are using DesNet from CNN3.py\n",
    "\n",
    "model = DesNet()\n",
    "# check model architecture\n",
    "\n",
    "print(model)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='set_opt_cell'></a>\n",
    "### Define optimization\n",
    "We will use SGD, but you can change it to ADAM by modifying arg.lr in config_profile.py\n",
    "\n",
    "[BackToSection](#build_network_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    else:\n",
    "        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))\n",
    "    return optimizer\n",
    "optimizer1 = create_optimizer(model.features, args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<a id='generate_deep_features_cell'></a>\n",
    "## Generate deep features\n",
    "In this section, we will use your trained network to generate deep features for each patch:\n",
    "#### 1. [load weights](#load_weights_module_cell)\n",
    "#### 2. [load patches](#load_raw_patch_files_module_cell)\n",
    "#### 3. [get deep features](#get_deep_features_module_cell)\n",
    "\n",
    "\n",
    "\n",
    "[BackToTop](#top_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_weights_module_cell'></a>\n",
    "### Load network weights\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.3, inplace=False)\n",
       "    (7): Conv2d(128, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weight_path = \"models/liberty_train/_liberty_min_as_fliprot/CNN1.pth\" # suppose you select  checkpoint_4.pth as the best model for this architecture\n",
    "#trained_weight_path = \"class_checkpoint.pth\"\n",
    "test_model = DesNet()\n",
    "if args.cuda:\n",
    "    test_model.cuda()\n",
    "trained_weight = torch.load(trained_weight_path)['state_dict']\n",
    "test_model.load_state_dict(trained_weight)\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_raw_patch_files_module_cell'></a>\n",
    "### Load raw patch files\n",
    "Assume that the raw patch file is stored as patches.pth\n",
    "\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 20, 1, 32, 32])\n",
      "torch.Size([2720, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "patches_dir = \"Hw2/Images_patches.pth\"            # these patches are from keypoint detection results\n",
    "patches = torch.load(patches_dir)\n",
    "\n",
    "print(patches.shape)                  # in your case, the shape should be [10, 200, 1, 32, 32]\n",
    "num_imgs, num_pts, _, _, _ = patches.shape\n",
    "patches = patches.view(-1, 1, 32, 32).cuda()\n",
    "print(patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get_deep_features_module_cell'></a>\n",
    "### Get deep features\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2720, 128])\n",
      "torch.Size([136, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "features = test_model(patches)\n",
    "print(features.shape)\n",
    "features = features.view(num_imgs, num_pts, 128).cpu().data\n",
    "print(features.shape)                  # in your case, the shape should be [10, 200, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file, with the name of *_features_CNN*.pth\n",
    "features_dir = \"Hw2/Images_features_CNN1.pth\"\n",
    "#features_dir = \"Hw2/Images_features_class_CNN3.pth\"\n",
    "torch.save(features, features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 20, 128])\n",
      "torch.Size([34, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "patches_dir = \"Hw2/Images_features_CNN1.pth\" \n",
    "#patches_dir = \"Hw2/Images_features_class_CNN3.pth\"            # these patches are from keypoint detection results\n",
    "features_Images = torch.load(patches_dir)\n",
    "\n",
    "patches_dir = \"Hw2/Query_features_CNN1.pth\" \n",
    "#patches_dir = \"Hw2/Query_features_class_CNN3.pth\" \n",
    "features_Query = torch.load(patches_dir)\n",
    "\n",
    "print(features_Images.shape)\n",
    "print(features_Query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_Images\n",
    "#features_Query\n",
    "\n",
    "def manytomany(features_Query, features_Images, threshold):\n",
    "    sqr = np.zeros((34,136))\n",
    "    for num_Q in range(len(features_Query)):\n",
    "        for num_I in range(len(features_Images)):         \n",
    "            skl = []\n",
    "            x = 0\n",
    "            for Q_keypoint in range(20):\n",
    "                for I_keypoint in range(20):                        \n",
    "                        #similarity = 0.5 *( 1 + \n",
    "                        #            ( features_Query[num_Q][Q_keypoint].dot(features_Images[num_I][I_keypoint]) ) / \n",
    "                        #            ( norm(features_Query[num_Q][Q_keypoint],2) * norm(features_Images[num_I][I_keypoint],2) ) )\n",
    "                        similarity = F.cosine_similarity(features_Query[num_Q][Q_keypoint], features_Images[num_I][I_keypoint], dim=0)\n",
    "                        skl.append(similarity)\n",
    "            x = skl / norm(skl,2)\n",
    "            # check the threshold\n",
    "            for x_idx in range(20 * 20):\n",
    "                if x[x_idx] > threshold:\n",
    "                    x[x_idx] = 1\n",
    "                else:\n",
    "                    x[x_idx] = 0\n",
    "            # skl.dot(x) is the sum of similarity of matching from 1 query to 1 image\n",
    "            skl = np.array(skl)\n",
    "            x = np.array(x)\n",
    "            \n",
    "            sqr[num_Q][num_I] = skl.dot(x)\n",
    "    return sqr\n",
    "\n",
    "def get_indicator_matrix(cost_matrix):\n",
    "    x = np.zeros((20,20))\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    for i in range(20):\n",
    "        x[i][col_ind[i]] = 1\n",
    "    return x\n",
    "    \n",
    "    \n",
    "    #trace = cost_matrix[row_ind, col_ind].sum()   \n",
    "    #return trace\n",
    "\n",
    "def onetoone(features_Query, features_Images):\n",
    "    sqr = np.zeros((34,136))\n",
    "    for num_Q in range(len(features_Query)):\n",
    "        for num_I in range(len(features_Images)):\n",
    "            ckl = np.zeros((20,20))\n",
    "            for Q_keypoint in range(20):\n",
    "                for I_keypoint in range(20):\n",
    "                        #dist = 1 - 0.5 *( 1 + \n",
    "                        #        ( features_Query[num_Q][Q_keypoint].dot(features_Images[num_I][I_keypoint]) ) / \n",
    "                        #        ( norm(features_Query[num_Q][Q_keypoint],2) * norm(features_Images[num_I][I_keypoint],2) ) )\n",
    "                        dist = 1 - F.cosine_similarity(features_Query[num_Q][Q_keypoint], features_Images[num_I][I_keypoint], dim=0)\n",
    "                        ckl[Q_keypoint][I_keypoint] = dist\n",
    "            indicator_matrix = get_indicator_matrix(ckl)\n",
    "            skl = 1 - ckl\n",
    "            diagonal_value = (skl.transpose()).dot(indicator_matrix)\n",
    "            max_trace = 0\n",
    "            for i in range(20):\n",
    "                max_trace = max_trace + diagonal_value[i][i]\n",
    "            \n",
    "            #sqr.append(min_trace_value)\n",
    "            sqr[num_Q][num_I] = max_trace\n",
    "    return sqr \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_CNN_one_to_one_match = onetoone(features_Query, features_Images)\n",
    "My_CNN_many_to_many_match = manytomany(features_Query,features_Images, 0.072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_CNN_one_to_one_match = onetoone(features_Query, features_Images)\n",
    "Class_CNN_many_to_many_match = manytomany(features_Query,features_Images, 0.072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.56645739 14.04885882 13.51514792 ...  3.6626125   3.58839577\n",
      "   6.49759161]\n",
      " [ 8.95349592  7.29209548  6.16056126 ...  6.35705376  9.13467175\n",
      "   6.83155906]\n",
      " [ 8.35182863  8.46824199  8.25339073 ...  7.78132105  8.23193669\n",
      "  10.10796839]\n",
      " ...\n",
      " [ 8.42697036  9.16479039  9.57609785 ...  5.27645582  2.18597549\n",
      "   6.70704353]\n",
      " [ 8.17721921  8.33487839  8.88144207 ...  6.46836996  6.68150651\n",
      "   7.99863952]\n",
      " [ 3.9529438   3.50869364  3.55378795 ... 10.26110089 11.33923626\n",
      "   9.26212895]]\n",
      "[[70.01556396 67.33731079 55.52961349 ... 14.43108368  3.11550856\n",
      "  17.97827721]\n",
      " [38.27606201 26.00609207 24.09695435 ... 21.73810387 56.17092133\n",
      "  24.0947876 ]\n",
      " [36.20726013 37.34459686 34.6930542  ... 27.88594437 31.45701408\n",
      "  26.24356079]\n",
      " ...\n",
      " [34.58694077 39.43255615 48.94757462 ... 16.09321022  8.04718113\n",
      "  20.71003532]\n",
      " [31.71372604 31.1531601  35.96695328 ... 21.6979351  16.3176918\n",
      "  21.8795948 ]\n",
      " [ 4.41961956  6.42295074  4.23182964 ... 33.30474472 66.27648926\n",
      "  21.29030609]]\n",
      "[[12.84781688 12.34827054 11.13495606 ...  1.74999815  1.84013945\n",
      "   2.84305531]\n",
      " [ 5.43247765  5.0852496   4.28190565 ...  4.22379059  6.75863254\n",
      "   4.34247464]\n",
      " [ 4.96475106  5.05502838  4.80411094 ...  4.83080906  4.46879226\n",
      "   5.19693661]\n",
      " ...\n",
      " [ 3.67180443  3.42231083  3.26674336 ...  4.52758211  3.56958556\n",
      "   4.42033356]\n",
      " [ 3.17516822  3.13744378  3.18161327 ...  4.20853466  4.29058385\n",
      "   4.87223339]\n",
      " [ 2.11272591  2.20560998  1.87768304 ...  8.10546666  9.03669095\n",
      "   5.99262637]]\n",
      "[[54.94814682 49.65885925 43.39214706 ...  5.2998476   5.83444309\n",
      "   7.06222248]\n",
      " [19.09688187 16.59911346 13.01546097 ... 12.44078827 40.10110474\n",
      "  13.71834087]\n",
      " [15.97013283 19.16454315 18.51675797 ... 18.90492821 17.71614647\n",
      "  14.9080925 ]\n",
      " ...\n",
      " [14.46895981 11.06453991 11.88620663 ... 17.76514244 15.13380718\n",
      "  16.98770905]\n",
      " [11.96338081  8.90481853 10.14943695 ... 14.51511478 18.06370354\n",
      "  14.81122303]\n",
      " [ 4.98198128  6.54551888  4.13031054 ... 26.63555527 39.15530777\n",
      "  17.27101898]]\n"
     ]
    }
   ],
   "source": [
    "print(My_CNN_one_to_one_match)\n",
    "print(My_CNN_many_to_many_match)\n",
    "print(Class_CNN_one_to_one_match)\n",
    "print(Class_CNN_many_to_many_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 4])\n"
     ]
    }
   ],
   "source": [
    "#Run this block after got the above four value.\n",
    "final_retrieval = []\n",
    "for j in range(4):    \n",
    "    if j == 0:\n",
    "        test_tensor = torch.Tensor(My_CNN_many_to_many_match)\n",
    "    if j == 1:\n",
    "        test_tensor = torch.Tensor(My_CNN_one_to_one_match)\n",
    "    if j == 2:\n",
    "        test_tensor = torch.Tensor(Class_CNN_many_to_many_match)\n",
    "    if j == 3:\n",
    "        test_tensor = torch.Tensor(Class_CNN_one_to_one_match)        \n",
    "    retrieval_matrix = torch.Tensor(np.zeros((34,4)))\n",
    "    for i in range(34):\n",
    "        R = torch.topk(test_tensor[i], 4, dim=0, largest=True, sorted=True, out=None)\n",
    "        retrieval_matrix[i] = R[1]\n",
    "    final_retrieval.append(retrieval_matrix)    \n",
    "all_retrieval_matrix = torch.stack(final_retrieval, dim=0)\n",
    "print(all_retrieval_matrix.shape)\n",
    "\n",
    "retrieval_pth = \"Hw2/retrieval.pth\" \n",
    "torch.save(all_retrieval_matrix,retrieval_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision (test_method, k):\n",
    "    precision_list = []\n",
    "    test_tensor = torch.Tensor(test_method)\n",
    "    for query_num in range(34):\n",
    "        #print(query_num)\n",
    "        true_positive = 0\n",
    "        Top_K_idx = torch.topk(test_tensor[query_num], k, dim=0, largest=True, sorted=True, out=None)\n",
    "        for k_idx in range(k):\n",
    "            #print(Top_K_idx[1][k_idx])\n",
    "            if (Top_K_idx[1][k_idx] == 4 * query_num) or (Top_K_idx[1][k_idx] == 4 * query_num + 1) or (Top_K_idx[1][k_idx] == 4 * query_num + 2) or (Top_K_idx[1][k_idx] == 4 * query_num + 3):\n",
    "                true_positive = true_positive + 1\n",
    "        precision_list.append(true_positive / k)\n",
    "    precision_value = sum(precision_list) / 34\n",
    "    return precision_value\n",
    "#print(precision_value)\n",
    "\n",
    "def Recall (test_method, k):\n",
    "    recall_list = []\n",
    "    test_tensor = torch.Tensor(test_method)\n",
    "    for query_num in range(34):\n",
    "        #print(query_num)\n",
    "        true_positive = 0\n",
    "        Top_K_idx = torch.topk(test_tensor[query_num], k, dim=0, largest=True, sorted=True, out=None)\n",
    "        for k_idx in range(k):\n",
    "            #print(Top_K_idx[1][k_idx])\n",
    "            if (Top_K_idx[1][k_idx] == 4 * query_num) or (Top_K_idx[1][k_idx] == 4 * query_num + 1) or (Top_K_idx[1][k_idx] == 4 * query_num + 2) or (Top_K_idx[1][k_idx] == 4 * query_num + 3):\n",
    "                true_positive = true_positive + 1\n",
    "        recall_list.append(true_positive / 4) # 4 is the ground truth in each image\n",
    "    recall_value = sum(recall_list) / 34\n",
    "    return recall_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "T = torch.cat((features_Query, features_Images))\n",
    "print(T.shape)\n",
    "filename = \"Hw2/features1.pth\" #feature1 is my CNN, feature2 is class CNN\n",
    "torch.save(T,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882352941176471\n",
      "0.5294117647058824\n",
      "0.45098039215686275\n",
      "0.40441176470588236\n",
      "-------\n",
      "0.14705882352941177\n",
      "0.2647058823529412\n",
      "0.3382352941176471\n",
      "0.40441176470588236\n",
      "-------\n",
      "0.29411764705882354\n",
      "0.20588235294117646\n",
      "0.196078431372549\n",
      "0.19852941176470587\n",
      "-------\n",
      "0.07352941176470588\n",
      "0.10294117647058823\n",
      "0.14705882352941177\n",
      "0.19852941176470587\n"
     ]
    }
   ],
   "source": [
    "print(Precision(My_CNN_one_to_one_match,1))\n",
    "print(Precision(My_CNN_one_to_one_match,2))\n",
    "print(Precision(My_CNN_one_to_one_match,3))\n",
    "print(Precision(My_CNN_one_to_one_match,4))\n",
    "print('-------')\n",
    "print(Recall(My_CNN_one_to_one_match,1))\n",
    "print(Recall(My_CNN_one_to_one_match,2))\n",
    "print(Recall(My_CNN_one_to_one_match,3))\n",
    "print(Recall(My_CNN_one_to_one_match,4))\n",
    "print('-------')\n",
    "print(Precision(My_CNN_many_to_many_match,1))\n",
    "print(Precision(My_CNN_many_to_many_match,2))\n",
    "print(Precision(My_CNN_many_to_many_match,3))\n",
    "print(Precision(My_CNN_many_to_many_match,4))\n",
    "print('-------')\n",
    "print(Recall(My_CNN_many_to_many_match,1))\n",
    "print(Recall(My_CNN_many_to_many_match,2))\n",
    "print(Recall(My_CNN_many_to_many_match,3))\n",
    "print(Recall(My_CNN_many_to_many_match,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9705882352941176\n",
      "0.8823529411764706\n",
      "0.7549019607843137\n",
      "0.6764705882352942\n",
      "-------\n",
      "0.2426470588235294\n",
      "0.4411764705882353\n",
      "0.5661764705882353\n",
      "0.6764705882352942\n",
      "-------\n",
      "0.7352941176470589\n",
      "0.5588235294117647\n",
      "0.45098039215686275\n",
      "0.4264705882352941\n",
      "-------\n",
      "0.18382352941176472\n",
      "0.27941176470588236\n",
      "0.3382352941176471\n",
      "0.4264705882352941\n"
     ]
    }
   ],
   "source": [
    "print(Precision(Class_CNN_one_to_one_match,1))\n",
    "print(Precision(Class_CNN_one_to_one_match,2))\n",
    "print(Precision(Class_CNN_one_to_one_match,3))\n",
    "print(Precision(Class_CNN_one_to_one_match,4))\n",
    "print('-------')\n",
    "print(Recall(Class_CNN_one_to_one_match,1))\n",
    "print(Recall(Class_CNN_one_to_one_match,2))\n",
    "print(Recall(Class_CNN_one_to_one_match,3))\n",
    "print(Recall(Class_CNN_one_to_one_match,4))\n",
    "print('-------')\n",
    "print(Precision(Class_CNN_many_to_many_match,1))\n",
    "print(Precision(Class_CNN_many_to_many_match,2))\n",
    "print(Precision(Class_CNN_many_to_many_match,3))\n",
    "print(Precision(Class_CNN_many_to_many_match,4))\n",
    "print('-------')\n",
    "print(Recall(Class_CNN_many_to_many_match,1))\n",
    "print(Recall(Class_CNN_many_to_many_match,2))\n",
    "print(Recall(Class_CNN_many_to_many_match,3))\n",
    "print(Recall(Class_CNN_many_to_many_match,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n",
      "[0 1 2]\n",
      "(array([0, 1, 2]), array([1, 2, 0]))\n",
      "[1 5 3]\n",
      "9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 4])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([ 4,  6, 55, 54])\n",
      "tensor([109,  89,  87,  57])\n",
      "tensor([ 13,  12, 127, 119])\n",
      "tensor([ 16,  40, 107,  99])\n",
      "tensor([29, 22, 99, 31])\n",
      "tensor([52, 53, 72, 55])\n",
      "tensor([ 28,  30, 114, 113])\n",
      "tensor([109,  34,  89,  87])\n",
      "tensor([ 36,  99, 107,  29])\n",
      "tensor([42, 40, 41, 43])\n",
      "tensor([ 57,  56,  46, 102])\n",
      "tensor([116,  72,  94,  62])\n",
      "tensor([55, 54,  6,  4])\n",
      "tensor([109,  98,  56, 107])\n",
      "tensor([61, 60, 63, 62])\n",
      "tensor([66, 64, 67, 41])\n",
      "tensor([ 69, 101, 100, 103])\n",
      "tensor([72, 95, 73, 61])\n",
      "tensor([76, 22, 21, 94])\n",
      "tensor([91, 46, 83, 90])\n",
      "tensor([135, 133, 132,  84])\n",
      "tensor([89, 72, 90, 75])\n",
      "tensor([75, 74, 76, 72])\n",
      "tensor([ 97, 122, 123,  37])\n",
      "tensor([100, 101, 103, 102])\n",
      "tensor([ 43,   4,  55, 107])\n",
      "tensor([42, 41, 43, 66])\n",
      "tensor([112, 114, 110, 113])\n",
      "tensor([119, 116,  40,   4])\n",
      "tensor([122,  41,  21,  43])\n",
      "tensor([ 14, 125, 116, 124])\n",
      "tensor([ 87, 129, 108, 128])\n",
      "tensor([132, 134,  83,  58])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
