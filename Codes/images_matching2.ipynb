{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64\n",
    "from Utils import L2Norm, cv2_scale, np_reshape\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability, using nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Since there are two GPUs on each pelican server, you can either select it as 0 or 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<a id='load_data_cell'></a>\n",
    "## Load and visualize the data\n",
    "\n",
    "In this section, we will \n",
    "#### 1. [Define a PyTorch dataset](#pytorch_dataset_cell)\n",
    "#### 2. [Define a PyTorch dataloader](#pytorch_dataloader_cell)\n",
    "#### 3. [Load data](#load_dataset_cell)\n",
    "#### 4. [Visualizaiton of the Training and Testing Data](#visualize_dataset_cell)\n",
    "\n",
    "[BackToTop](#top_cell)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_dataset_cell'></a>\n",
    "### Define PyTorch dataset\n",
    "\n",
    "[BackToSection](#load_data_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletPhotoTour(dset.PhotoTour):\n",
    "    \"\"\"\n",
    "    From the PhotoTour Dataset it generates triplet samples\n",
    "    note: a triplet is composed by a pair of matching images and one of\n",
    "    different class.\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, transform=None, batch_size = None,load_random_triplets = False,  *arg, **kw):\n",
    "        super(TripletPhotoTour, self).__init__(*arg, **kw)\n",
    "        self.transform = transform\n",
    "        self.out_triplets = load_random_triplets\n",
    "        self.train = train\n",
    "        self.n_triplets = args.n_triplets\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n",
    "            \n",
    "    @staticmethod\n",
    "    def generate_triplets(labels, num_triplets):\n",
    "        def create_indices(_labels):\n",
    "            inds = dict()\n",
    "            for idx, ind in enumerate(_labels):\n",
    "                if ind not in inds:\n",
    "                    inds[ind] = []\n",
    "                inds[ind].append(idx)\n",
    "            return inds\n",
    "\n",
    "        triplets = []\n",
    "        indices = create_indices(labels.numpy())\n",
    "        unique_labels = np.unique(labels.numpy())\n",
    "        n_classes = unique_labels.shape[0]\n",
    "        # add only unique indices in batch\n",
    "        already_idxs = set()\n",
    "\n",
    "        for x in tqdm(range(num_triplets)):\n",
    "            if len(already_idxs) >= args.batch_size:\n",
    "                already_idxs = set()\n",
    "            c1 = np.random.randint(0, n_classes)\n",
    "            while c1 in already_idxs:\n",
    "                c1 = np.random.randint(0, n_classes)\n",
    "            already_idxs.add(c1)\n",
    "            c2 = np.random.randint(0, n_classes)\n",
    "            while c1 == c2:\n",
    "                c2 = np.random.randint(0, n_classes)\n",
    "            if len(indices[c1]) == 2:  # hack to speed up process\n",
    "                n1, n2 = 0, 1\n",
    "            else:\n",
    "                n1 = np.random.randint(0, len(indices[c1]))\n",
    "                n2 = np.random.randint(0, len(indices[c1]))\n",
    "                while n1 == n2:\n",
    "                    n2 = np.random.randint(0, len(indices[c1]))\n",
    "            n3 = np.random.randint(0, len(indices[c2]))\n",
    "            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def transform_img(img):\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img.numpy())\n",
    "            return img\n",
    "\n",
    "        t = self.triplets[index]\n",
    "        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n",
    "\n",
    "        img_a = transform_img(a)\n",
    "        img_p = transform_img(p)\n",
    "        img_n = None\n",
    "        if self.out_triplets:\n",
    "            img_n = transform_img(n)\n",
    "        # transform images if required\n",
    "        if args.fliprot:\n",
    "            do_flip = random.random() > 0.5\n",
    "            do_rot = random.random() > 0.5\n",
    "            if do_rot:\n",
    "                img_a = img_a.permute(0,2,1)\n",
    "                img_p = img_p.permute(0,2,1)\n",
    "                if self.out_triplets:\n",
    "                    img_n = img_n.permute(0,2,1)\n",
    "            if do_flip:\n",
    "                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n",
    "                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n",
    "                if self.out_triplets:\n",
    "                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n",
    "        return (img_a, img_p, img_n)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triplets.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch_dataloader_cell'></a>\n",
    "### Define the dataloader\n",
    "[BackToSection](#load_data_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset_names, load_random_triplets = False, verbose=False):\n",
    "    \"\"\"\n",
    "    For training, we use dataset 'liberty';\n",
    "    For testing, we use dataset 'notredame' and 'yosemite'\n",
    "    \n",
    "    \"\"\"\n",
    "    test_dataset_names = copy(dataset_names)\n",
    "    test_dataset_names.remove(args.training_set)\n",
    "\n",
    "    kwargs = {'num_workers': args.num_workers, 'pin_memory': args.pin_memory} if args.cuda else {}\n",
    "\n",
    "    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n",
    "            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Lambda(cv2_scale),\n",
    "            transforms.Lambda(np_reshape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((args.mean_image,), (args.std_image,))])\n",
    "    if not args.augmentation:\n",
    "        transform_train = transform\n",
    "        transform_test = transform\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            TripletPhotoTour(train=True,\n",
    "                             load_random_triplets = load_random_triplets,\n",
    "                             batch_size=args.batch_size,\n",
    "                             root=args.dataroot,\n",
    "                             name=args.training_set,\n",
    "                             download=True,\n",
    "                             transform=transform_train),\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, **kwargs)\n",
    "\n",
    "    test_loaders = [{'name': name,\n",
    "                     'dataloader': torch.utils.data.DataLoader(\n",
    "             TripletPhotoTour(train=False,\n",
    "                     batch_size=args.test_batch_size,\n",
    "                     load_random_triplets = load_random_triplets,\n",
    "                     root=args.dataroot,\n",
    "                     name=name,\n",
    "                     download=True,\n",
    "                     transform=transform_test),\n",
    "                        batch_size=args.test_batch_size,\n",
    "                        shuffle=False, **kwargs)}\n",
    "                    for name in test_dataset_names]\n",
    "\n",
    "    return train_loader, test_loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='build_network_cell'></a>\n",
    "## Build an exmaple deep network  \n",
    "\n",
    "In this section, we will:\n",
    "#### 1. [Build the deep network: DesNet](#build_desNet_cell)\n",
    "#### 2. [Setup optimization](#set_opt_cell)\n",
    "\n",
    "[BackToTop](#top_cell)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='build_desNet_cell'></a>\n",
    "### Build the deep network: DesNet\n",
    "The DesNet is a simple CNN network, which only contains two CNN blocks.\n",
    "\n",
    "\n",
    "[BackToSection](#build_network_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DesNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load network from the python file. You need to submit these .py files to TA\n",
    "from CNN1 import DesNet       # uncomment this line if you are using DesNet from CNN1.py\n",
    "#from CNN2 import DesNet      # uncomment this line if you are using DesNet from CNN2.py\n",
    "#from CNN3 import DesNet\n",
    "#from class_CNN3 import DesNet      # uncomment this line if you are using DesNet from CNN3.py\n",
    "\n",
    "model = DesNet()\n",
    "# check model architecture\n",
    "\n",
    "print(model)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='set_opt_cell'></a>\n",
    "### Define optimization\n",
    "We will use SGD, but you can change it to ADAM by modifying arg.lr in config_profile.py\n",
    "\n",
    "[BackToSection](#build_network_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    else:\n",
    "        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))\n",
    "    return optimizer\n",
    "optimizer1 = create_optimizer(model.features, args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='generate_deep_features_cell'></a>\n",
    "## Generate deep features\n",
    "In this section, we will use your trained network to generate deep features for each patch:\n",
    "#### 1. [load weights](#load_weights_module_cell)\n",
    "#### 2. [load patches](#load_raw_patch_files_module_cell)\n",
    "#### 3. [get deep features](#get_deep_features_module_cell)\n",
    "\n",
    "\n",
    "\n",
    "[BackToTop](#top_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_weights_module_cell'></a>\n",
    "### Load network weights\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.3, inplace=False)\n",
       "    (7): Conv2d(128, 128, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weight_path = \"models/liberty_train/_liberty_min_as_fliprot/CNN1.pth\" # suppose you select  checkpoint_4.pth as the best model for this architecture\n",
    "#trained_weight_path = \"class_checkpoint.pth\"\n",
    "test_model = DesNet()\n",
    "if args.cuda:\n",
    "    test_model.cuda()\n",
    "trained_weight = torch.load(trained_weight_path)['state_dict']\n",
    "test_model.load_state_dict(trained_weight)\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_raw_patch_files_module_cell'></a>\n",
    "### Load raw patch files\n",
    "Assume that the raw patch file is stored as patches.pth\n",
    "\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 20, 1, 32, 32])\n",
      "torch.Size([2720, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "patches_dir = \"cd ../My_Results/Query_Images_Matching/Images_patches.pth\"            # these patches are from keypoint detection results\n",
    "patches = torch.load(patches_dir)\n",
    "\n",
    "print(patches.shape)                  # in your case, the shape should be [10, 200, 1, 32, 32]\n",
    "num_imgs, num_pts, _, _, _ = patches.shape\n",
    "patches = patches.view(-1, 1, 32, 32).cuda()\n",
    "print(patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get_deep_features_module_cell'></a>\n",
    "### Get deep features\n",
    "[BackToSection](#generate_deep_features_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2720, 128])\n",
      "torch.Size([136, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "features = test_model(patches)\n",
    "print(features.shape)\n",
    "features = features.view(num_imgs, num_pts, 128).cpu().data\n",
    "print(features.shape)                  # in your case, the shape should be [10, 200, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file, with the name of *_features_CNN*.pth\n",
    "features_dir = \"cd ../My_Results/Query_Images_Matching/Images_features_CNN1.pth\"\n",
    "#features_dir = \"cd ../My_Results/Query_Images_Matching/Images_features_class_CNN3.pth\"\n",
    "torch.save(features, features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 20, 128])\n",
      "torch.Size([34, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "patches_dir = \"cd ../My_Results/Query_Images_Matching/Images_features_CNN1.pth\" \n",
    "#patches_dir = \"cd ../My_Results/Query_Images_Matching/Images_features_class_CNN3.pth\"            # these patches are from keypoint detection results\n",
    "features_Images = torch.load(patches_dir)\n",
    "\n",
    "patches_dir = \"cd ../My_Results/Query_Images_Matching/Query_features_CNN1.pth\" \n",
    "#patches_dir = \"cd ../My_Results/Query_Images_Matching/Query_features_class_CNN3.pth\" \n",
    "features_Query = torch.load(patches_dir)\n",
    "\n",
    "print(features_Images.shape)\n",
    "print(features_Query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_Images\n",
    "#features_Query\n",
    "\n",
    "def manytomany(features_Query, features_Images, threshold):\n",
    "    sqr = np.zeros((34,136))\n",
    "    for num_Q in range(len(features_Query)):\n",
    "        for num_I in range(len(features_Images)):         \n",
    "            skl = []\n",
    "            x = 0\n",
    "            for Q_keypoint in range(20):\n",
    "                for I_keypoint in range(20):                        \n",
    "                        #similarity = 0.5 *( 1 + \n",
    "                        #            ( features_Query[num_Q][Q_keypoint].dot(features_Images[num_I][I_keypoint]) ) / \n",
    "                        #            ( norm(features_Query[num_Q][Q_keypoint],2) * norm(features_Images[num_I][I_keypoint],2) ) )\n",
    "                        similarity = F.cosine_similarity(features_Query[num_Q][Q_keypoint], features_Images[num_I][I_keypoint], dim=0)\n",
    "                        skl.append(similarity)\n",
    "            x = skl / norm(skl,2)\n",
    "            # check the threshold\n",
    "            for x_idx in range(20 * 20):\n",
    "                if x[x_idx] > threshold:\n",
    "                    x[x_idx] = 1\n",
    "                else:\n",
    "                    x[x_idx] = 0\n",
    "            # skl.dot(x) is the sum of similarity of matching from 1 query to 1 image\n",
    "            skl = np.array(skl)\n",
    "            x = np.array(x)\n",
    "            \n",
    "            sqr[num_Q][num_I] = skl.dot(x)\n",
    "    return sqr\n",
    "\n",
    "def get_indicator_matrix(cost_matrix):\n",
    "    x = np.zeros((20,20))\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    for i in range(20):\n",
    "        x[i][col_ind[i]] = 1\n",
    "    return x\n",
    "    \n",
    "    \n",
    "    #trace = cost_matrix[row_ind, col_ind].sum()   \n",
    "    #return trace\n",
    "\n",
    "def onetoone(features_Query, features_Images):\n",
    "    sqr = np.zeros((34,136))\n",
    "    for num_Q in range(len(features_Query)):\n",
    "        for num_I in range(len(features_Images)):\n",
    "            ckl = np.zeros((20,20))\n",
    "            for Q_keypoint in range(20):\n",
    "                for I_keypoint in range(20):\n",
    "                        #dist = 1 - 0.5 *( 1 + \n",
    "                        #        ( features_Query[num_Q][Q_keypoint].dot(features_Images[num_I][I_keypoint]) ) / \n",
    "                        #        ( norm(features_Query[num_Q][Q_keypoint],2) * norm(features_Images[num_I][I_keypoint],2) ) )\n",
    "                        dist = 1 - F.cosine_similarity(features_Query[num_Q][Q_keypoint], features_Images[num_I][I_keypoint], dim=0)\n",
    "                        ckl[Q_keypoint][I_keypoint] = dist\n",
    "            indicator_matrix = get_indicator_matrix(ckl)\n",
    "            skl = 1 - ckl\n",
    "            diagonal_value = (skl.transpose()).dot(indicator_matrix)\n",
    "            max_trace = 0\n",
    "            for i in range(20):\n",
    "                max_trace = max_trace + diagonal_value[i][i]\n",
    "            \n",
    "            #sqr.append(min_trace_value)\n",
    "            sqr[num_Q][num_I] = max_trace\n",
    "    return sqr \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_CNN_one_to_one_match = onetoone(features_Query, features_Images)\n",
    "My_CNN_many_to_many_match = manytomany(features_Query,features_Images, 0.072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_CNN_one_to_one_match = onetoone(features_Query, features_Images)\n",
    "Class_CNN_many_to_many_match = manytomany(features_Query,features_Images, 0.072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 4])\n"
     ]
    }
   ],
   "source": [
    "#Run this block after got the above four value.\n",
    "final_retrieval = []\n",
    "for j in range(4):    \n",
    "    if j == 0:\n",
    "        test_tensor = torch.Tensor(My_CNN_many_to_many_match)\n",
    "    if j == 1:\n",
    "        test_tensor = torch.Tensor(My_CNN_one_to_one_match)\n",
    "    if j == 2:\n",
    "        test_tensor = torch.Tensor(Class_CNN_many_to_many_match)\n",
    "    if j == 3:\n",
    "        test_tensor = torch.Tensor(Class_CNN_one_to_one_match)        \n",
    "    retrieval_matrix = torch.Tensor(np.zeros((34,4)))\n",
    "    for i in range(34):\n",
    "        R = torch.topk(test_tensor[i], 4, dim=0, largest=True, sorted=True, out=None)\n",
    "        retrieval_matrix[i] = R[1]\n",
    "    final_retrieval.append(retrieval_matrix)    \n",
    "all_retrieval_matrix = torch.stack(final_retrieval, dim=0)\n",
    "print(all_retrieval_matrix.shape)\n",
    "\n",
    "retrieval_pth = \"cd ../My_Results/Query_Images_Matching/retrieval.pth\" \n",
    "torch.save(all_retrieval_matrix,retrieval_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision (test_method, k):\n",
    "    precision_list = []\n",
    "    test_tensor = torch.Tensor(test_method)\n",
    "    for query_num in range(34):\n",
    "        #print(query_num)\n",
    "        true_positive = 0\n",
    "        Top_K_idx = torch.topk(test_tensor[query_num], k, dim=0, largest=True, sorted=True, out=None)\n",
    "        for k_idx in range(k):\n",
    "            #print(Top_K_idx[1][k_idx])\n",
    "            if (Top_K_idx[1][k_idx] == 4 * query_num) or (Top_K_idx[1][k_idx] == 4 * query_num + 1) or (Top_K_idx[1][k_idx] == 4 * query_num + 2) or (Top_K_idx[1][k_idx] == 4 * query_num + 3):\n",
    "                true_positive = true_positive + 1\n",
    "        precision_list.append(true_positive / k)\n",
    "    precision_value = sum(precision_list) / 34\n",
    "    return precision_value\n",
    "#print(precision_value)\n",
    "\n",
    "def Recall (test_method, k):\n",
    "    recall_list = []\n",
    "    test_tensor = torch.Tensor(test_method)\n",
    "    for query_num in range(34):\n",
    "        #print(query_num)\n",
    "        true_positive = 0\n",
    "        Top_K_idx = torch.topk(test_tensor[query_num], k, dim=0, largest=True, sorted=True, out=None)\n",
    "        for k_idx in range(k):\n",
    "            #print(Top_K_idx[1][k_idx])\n",
    "            if (Top_K_idx[1][k_idx] == 4 * query_num) or (Top_K_idx[1][k_idx] == 4 * query_num + 1) or (Top_K_idx[1][k_idx] == 4 * query_num + 2) or (Top_K_idx[1][k_idx] == 4 * query_num + 3):\n",
    "                true_positive = true_positive + 1\n",
    "        recall_list.append(true_positive / 4) # 4 is the ground truth in each image\n",
    "    recall_value = sum(recall_list) / 34\n",
    "    return recall_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "T = torch.cat((features_Query, features_Images))\n",
    "print(T.shape)\n",
    "filename = \"cd ../My_Results/Query_Images_Matching/features1.pth\" #feature1 is my CNN, feature2 is class CNN\n",
    "torch.save(T,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
